{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfb6b2d-b59a-4758-b53e-fc815c86dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/hau/Downloads/spark-3.5.3-bin-hadoop3\"\n",
    "os.environ[\"PYTHONPATH\"] = os.environ.get(\"PYTHONPATH\", \"\") + \":/home/hau/Downloads/spark-3.5.3-bin-hadoop3/python\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/home/hau/anaconda3/envs/myenv/bin/python3.9\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/home/hau/anaconda3/envs/myenv/bin/python3.9\"\n",
    "# Thêm gói Kafka vào biến môi trường\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 pyspark-shell\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e36ce3-b98b-4c2e-8df8-86c4a77ace88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col, to_json\n",
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "from pyspark.sql.functions import from_unixtime, to_timestamp, lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b136612-21fe-4c79-9bc9-717c9fb47869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame đã được ghi vào: /home/hau/train/data-generator/input/sensors.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "directory = '/home/hau/train/datasets/KETI'\n",
    "dataframes = {}\n",
    "dataframes_room = {}\n",
    "\n",
    "# Thứ tự cột thực tế khi đọc file (cần thay đổi nếu dữ liệu thực tế khác)\n",
    "columns = ['co2', 'humidity', 'light', 'pir', 'temperature']\n",
    "\n",
    "def create_separate_dataframes() -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary that includes room numbers as keys and dataframes per room as values.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        new_directory = os.path.join(directory, filename)\n",
    "\n",
    "        # Bỏ qua nếu không phải là thư mục hoặc là thư mục ẩn\n",
    "        if not os.path.isdir(new_directory) or filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        count = 0\n",
    "        for new_file in sorted(os.listdir(new_directory)):\n",
    "            f = os.path.join(new_directory, new_file)\n",
    "\n",
    "            # Bỏ qua các file ẩn\n",
    "            if new_file.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            # Tạo key cho dataframe, ví dụ: '656_pir'\n",
    "            sensor_type = columns[count]  # Lấy tên cột tương ứng (pir, humidity, co2, ...)\n",
    "            my_path = f\"{filename}_{sensor_type}\"\n",
    "            dataframes[my_path] = pd.read_csv(f, names=['ts_min_bignt', sensor_type])\n",
    "            count += 1\n",
    "\n",
    "        # Tạo dataframe tổng hợp cho từng phòng bằng cách merge các dataframe lại\n",
    "        try:\n",
    "            dataframes_room[filename] = reduce(\n",
    "                lambda left, right: pd.merge(left, right, on='ts_min_bignt', how='inner'),\n",
    "                [dataframes[f\"{filename}_{col}\"] for col in columns]\n",
    "            )\n",
    "            dataframes_room[filename]['room'] = filename  # Thêm cột 'room' để lưu số phòng\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e} - Có thể thiếu một file dữ liệu trong phòng {filename}\")\n",
    "\n",
    "    return dataframes_room\n",
    "\n",
    "def create_main_dataframe(separate_dataframes: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenates all per-room dataframes vertically to create a final dataframe.\n",
    "    \"\"\"\n",
    "    dataframes_to_concat = list(separate_dataframes.values())\n",
    "\n",
    "    # Nối tất cả các dataframe\n",
    "    df = pd.concat(dataframes_to_concat, ignore_index=True)\n",
    "    df = df.sort_values('ts_min_bignt')  # Sắp xếp theo timestamp\n",
    "\n",
    "    df.dropna(inplace=True)  # Xóa các dòng có giá trị NaN\n",
    "    df[\"event_ts_min\"] = pd.to_datetime(df[\"ts_min_bignt\"], unit='s')  # Chuyển đổi timestamp thành datetime\n",
    "    return df\n",
    "\n",
    "def write_main_dataframe(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Writes the final dataframe to the local CSV file.\n",
    "    \"\"\"\n",
    "    output_path = '/home/hau/train/data-generator/input/sensors.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"DataFrame đã được ghi vào: {output_path}\")\n",
    "\n",
    "# Thực thi các hàm\n",
    "separate_dataframes = create_separate_dataframes()\n",
    "if separate_dataframes:\n",
    "    main_df = create_main_dataframe(separate_dataframes)\n",
    "    write_main_dataframe(main_df)\n",
    "else:\n",
    "    print(\"Không có dữ liệu để xử lý.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40221c8-610c-49a4-8548-24feabfbc407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 12:47:33 WARN Utils: Your hostname, hau resolves to a loopback address: 127.0.1.1; using 192.168.81.128 instead (on interface ens33)\n",
      "24/12/07 12:47:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/hau/Downloads/spark-3.5.3-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hau/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hau/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6ad2aae9-5dd6-4e7f-a2ea-30e8a5b28f0e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.3 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 432ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.3 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6ad2aae9-5dd6-4e7f-a2ea-30e8a5b28f0e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/7ms)\n",
      "24/12/07 12:47:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/07 12:47:41 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 8, schema size: 7\n",
      "CSV file: file:///home/hau/train/data-generator/input/sensors.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+--------+------+----+-----------+----+-------------------+\n",
      "|ts_min_bignt|co2  |humidity|light |pir |temperature|room|event_ts_min       |\n",
      "+------------+-----+--------+------+----+-----------+----+-------------------+\n",
      "|1377299093  |387.0|52.75   |252.0 |0.0 |22.62      |511 |2013-08-24 06:04:53|\n",
      "|1377299097  |465.0|52.4    |165.0 |0.0 |22.8       |644 |2013-08-24 06:04:57|\n",
      "|1377299097  |175.0|50.32   |191.0 |0.0 |23.32      |648 |2013-08-24 06:04:57|\n",
      "|1377299097  |579.0|49.9    |176.0 |30.0|24.37      |656A|2013-08-24 06:04:57|\n",
      "|1377299101  |434.0|49.94   |11.0  |29.0|24.08      |564 |2013-08-24 06:05:01|\n",
      "|1377299101  |423.0|53.51   |3.0   |0.0 |23.11      |558 |2013-08-24 06:05:01|\n",
      "|1377299105  |347.0|50.03   |3.0   |0.0 |23.64      |664 |2013-08-24 06:05:05|\n",
      "|1377299105  |437.0|49.39   |148.0 |0.0 |24.14      |666 |2013-08-24 06:05:05|\n",
      "|1377299105  |538.0|46.49   |102.0 |30.0|25.26      |656B|2013-08-24 06:05:05|\n",
      "|1377299105  |421.0|49.06   |1977.0|0.0 |24.81      |668 |2013-08-24 06:05:05|\n",
      "|1377299108  |495.0|45.34   |97.0  |0.0 |23.94      |413 |2013-08-24 06:05:08|\n",
      "|1377299125  |383.0|52.75   |251.0 |0.0 |22.62      |511 |2013-08-24 06:05:25|\n",
      "|1377299125  |391.0|52.62   |204.0 |0.0 |23.46      |510 |2013-08-24 06:05:25|\n",
      "|1377299125  |629.0|52.84   |30.0  |19.0|23.06      |746 |2013-08-24 06:05:25|\n",
      "|1377299125  |655.0|48.9    |105.0 |0.0 |24.6       |776 |2013-08-24 06:05:25|\n",
      "|1377299126  |384.0|47.47   |311.0 |17.0|24.8       |734 |2013-08-24 06:05:26|\n",
      "|1377299126  |439.0|53.51   |4.0   |0.0 |23.11      |558 |2013-08-24 06:05:26|\n",
      "|1377299126  |559.0|49.9    |153.0 |30.0|24.37      |656A|2013-08-24 06:05:26|\n",
      "|1377299126  |390.0|53.1    |3.0   |0.0 |22.81      |552 |2013-08-24 06:05:26|\n",
      "|1377299126  |342.0|50.03   |3.0   |0.0 |23.64      |664 |2013-08-24 06:05:26|\n",
      "+------------+-----+--------+------+----+-----------+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "# Khởi tạo Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KETI Data\") \\\n",
    "    .config(\"spark.jars\", \"/home/hau/Downloads/tlcn/Real-time-IoT-Sensor-Data-Dashboard-main/elasticsearch-spark-30_2.12-8.0.0-beta1.jar\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.kafka:kafka-clients:3.5.3,\"\n",
    "            \"commons-httpclient:commons-httpclient:3.1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Định nghĩa schema\n",
    "schema = StructType([\n",
    "    StructField(\"ts_min_bignt\", StringType(), True),\n",
    "    StructField(\"co2\", FloatType(), True),\n",
    "    StructField(\"humidity\", FloatType(), True),\n",
    "    StructField(\"light\", FloatType(), True),\n",
    "    StructField(\"pir\", FloatType(), True),\n",
    "    StructField(\"temperature\", FloatType(), True),\n",
    "    StructField(\"room\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Đường dẫn tới tệp sensors.csv\n",
    "file_path = \"/home/hau/train/data-generator/input/sensors.csv\"\n",
    "\n",
    "# Đọc dữ liệu từ tệp sensors.csv\n",
    "final_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(schema) \\\n",
    "    .load(f\"file://{file_path}\")\n",
    "\n",
    "# Bổ sung cột event_ts_min bằng cách chuyển đổi ts_min_bignt thành kiểu Timestamp\n",
    "final_df = final_df.withColumn(\"event_ts_min\", from_unixtime(final_df[\"ts_min_bignt\"]).cast(TimestampType()))\n",
    "\n",
    "# Hiển thị dữ liệu\n",
    "final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23416f0-1fb7-4a1c-b09e-37a0207ca17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts_min_bignt: string (nullable = true)\n",
      " |-- co2: float (nullable = true)\n",
      " |-- humidity: float (nullable = true)\n",
      " |-- light: float (nullable = true)\n",
      " |-- pir: float (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- room: string (nullable = true)\n",
      " |-- event_ts_min: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8412d504-948f-437a-8fbe-cd5a57504fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [2024-12-07 12:47:43,843] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'dataframe-to-kafka' already exists.\n",
      " (org.apache.kafka.tools.TopicCommand)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "command = [\n",
    "    \"/home/hau/Downloads/kafka/bin/kafka-topics.sh\",\n",
    "    \"--bootstrap-server\", \"localhost:9092\",\n",
    "    \"--create\",\n",
    "    \"--topic\", \"dataframe-to-kafka\",\n",
    "    \"--partitions\", \"5\",\n",
    "    \"--replication-factor\", \"1\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=True)\n",
    "    print(\"Output:\", result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error:\", e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82bb90db-5464-4bc0-a8fd-add41024c9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 12:47:44 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 8, schema size: 7\n",
      "CSV file: file:///home/hau/train/data-generator/input/sensors.csv\n",
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sent to Kafka successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "# Định nghĩa Kafka bootstrap servers và topic\n",
    "kafka_bootstrap_servers = \"localhost:9092\"  # Thay bằng địa chỉ Kafka của bạn\n",
    "kafka_topic = \"dataframe-to-kafka\"  # Thay bằng topic Kafka mà bạn muốn gửi dữ liệu\n",
    "\n",
    "# Chuyển đổi DataFrame thành định dạng JSON\n",
    "final_df_with_json = final_df.withColumn(\"value\", to_json(struct([col(c) for c in final_df.columns])))\n",
    "\n",
    "# Gửi dữ liệu đến Kafka\n",
    "final_df_with_json.select(\"value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"topic\", kafka_topic) \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data sent to Kafka successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ad8dbe-bf7c-4c8c-97c6-94d95dd48aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79ce62f5-ca62-4a69-bd1e-5d85f4228143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9801/3473412299.py:1: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.delete(index='office-index', ignore=[400, 404])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='office-index', ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f614dcf-f54d-407c-8cc5-1f87e43decb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "office_index = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"custom_analyzer\":\n",
    "                        {\n",
    "                            \"type\": \"custom\",\n",
    "                            \"tokenizer\": \"standard\",\n",
    "                            \"filter\": [\n",
    "                                \"lowercase\", \"custom_edge_ngram\", \"asciifolding\"\n",
    "                            ]\n",
    "                        }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"custom_edge_ngram\": {\n",
    "                        \"type\": \"edge_ngram\",\n",
    "                        \"min_gram\": 2,\n",
    "                        \"max_gram\": 10\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"event_ts_min\": {\"type\": \"date\",\n",
    "            \"format\": \"yyyy-MM-d hh:mm:ss||yyyy-MM-dd hh:mm:ss||yyyy-MM-dd HH:mm:ss||yyyy-MM-d HH:mm:ss\",\n",
    "            \"ignore_malformed\": \"true\"\n",
    "      },\n",
    "            \"co2\": {\"type\": \"float\"},\n",
    "            \"humidity\": {\"type\": \"float\"},\n",
    "            \"light\": {\"type\": \"float\"},\n",
    "            \"temperature\": {\"type\": \"float\"},\n",
    "            \"room\": {\"type\": \"keyword\"},\n",
    "            \"pir\": {\"type\": \"float\"},\n",
    "            \"if_movement\": {\"type\": \"keyword\"}\n",
    "\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64c5ad49-8640-4e93-9562-97a538fd7f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'office-index'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=\"office-index\", body=office_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250d99f-da2a-421c-8cba-3f6ed0aee9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 12:47:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/12/07 12:47:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/12/07 12:47:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------+-----+-----------+----+----+-----------+\n",
      "|       event_ts_min|  co2|humidity|light|temperature|room| pir|if_movement|\n",
      "+-------------------+-----+--------+-----+-----------+----+----+-----------+\n",
      "|2013-08-28 14:41:48|473.0|   58.67|  5.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-28 14:41:58|480.0|   58.67|  3.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-28 14:42:08|467.0|   58.67|  3.0|      22.77| 446| 0.0|no_movement|\n",
      "|2013-08-24 06:17:20|459.0|   48.16|  4.0|      24.53| 726| 0.0|no_movement|\n",
      "|2013-08-28 14:42:18|464.0|   58.67|  4.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-28 14:42:28|464.0|   58.67|  5.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-24 06:17:22|543.0|   50.45|  6.0|      23.23| 748| 0.0|no_movement|\n",
      "|2013-08-28 14:42:38|472.0|   58.67|  3.0|      22.79| 446| 0.0|no_movement|\n",
      "|2013-08-24 06:17:23|677.0|   52.97|144.0|      23.47| 419|30.0|   movement|\n",
      "|2013-08-28 14:42:48|472.0|   58.67|  3.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-28 14:42:58|472.0|   58.67|  4.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-24 06:17:28|453.0|   49.74|152.0|      24.02| 666| 0.0|no_movement|\n",
      "|2013-08-28 14:43:08|469.0|   58.67|  4.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-28 14:43:18|469.0|   58.67|  3.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-28 14:43:28|480.0|   58.67|  4.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-28 14:43:38|467.0|   58.67|  4.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-28 14:43:48|464.0|   58.67|  3.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-24 06:17:28|682.0|   52.97|143.0|      23.48| 419|30.0|   movement|\n",
      "|2013-08-28 14:43:58|455.0|   58.64|  4.0|      22.78| 446| 0.0|no_movement|\n",
      "|2013-08-24 06:17:30|459.0|   48.16|  3.0|      24.54| 726| 0.0|no_movement|\n",
      "+-------------------+-----+--------+-----+-----------+----+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 12:48:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 10440 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamingToElasticsearch\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Định nghĩa schema cho dữ liệu JSON\n",
    "schema = StructType([\n",
    "    StructField(\"ts_min_bignt\", StringType(), True),\n",
    "    StructField(\"co2\", FloatType(), True),\n",
    "    StructField(\"humidity\", FloatType(), True),\n",
    "    StructField(\"light\", FloatType(), True),\n",
    "    StructField(\"pir\", FloatType(), True),\n",
    "    StructField(\"temperature\", FloatType(), True),\n",
    "    StructField(\"room\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"dataframe-to-kafka\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Chuyển đổi giá trị từ Kafka (dạng byte) thành chuỗi JSON\n",
    "df_parsed = df.selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .withColumn(\"data\", F.from_json(F.col(\"json_value\"), schema)) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Thêm cột event_ts_min (chuyển đổi từ ts_min_bignt sang timestamp)\n",
    "df_parsed = df_parsed.withColumn(\n",
    "    \"event_ts_min\", \n",
    "    F.from_unixtime(F.col(\"ts_min_bignt\").cast(\"long\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "# Áp dụng SELECT với logic CASE\n",
    "df_final = df_parsed.selectExpr(\n",
    "    \"event_ts_min\",\n",
    "    \"co2\",\n",
    "    \"humidity\",\n",
    "    \"light\",\n",
    "    \"temperature\",\n",
    "    \"room\",\n",
    "    \"pir\",\n",
    "    \"CASE WHEN pir > 0 THEN 'movement' ELSE 'no_movement' END as if_movement\"\n",
    ")\n",
    "\n",
    "# Hàm để ghi dữ liệu vào Elasticsearch\n",
    "def write_to_elasticsearch(batch_df, batch_id):\n",
    "    # Debug: In dữ liệu của batch\n",
    "    batch_df.show()\n",
    "\n",
    "    # Ghi vào Elasticsearch\n",
    "    batch_df.write \\\n",
    "        .format(\"org.elasticsearch.spark.sql\") \\\n",
    "        .option(\"es.resource\", \"office-index\") \\\n",
    "        .option(\"es.nodes\", \"localhost\") \\\n",
    "        .option(\"es.port\", \"9200\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "# Cấu hình Streaming Query và trigger mỗi 3 giây\n",
    "query = df_final.writeStream \\\n",
    "    .trigger(processingTime=\"3 seconds\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_elasticsearch) \\\n",
    "    .option(\"checkpointLocation\", \"/home/hau/Downloads/tlcn/Real-time-IoT-Sensor-Data-Dashboard-main/checkpoint/dir\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc5e03-d67a-4ec3-baa1-01afa8a4b547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c751908-ca39-4fda-be69-b1bf160228a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d53ee3-a8bc-4184-b9f0-160597295014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
